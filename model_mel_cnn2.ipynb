{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_melspectrogram(audio, sr, n_mels=128, n_fft=2048, hop_length=512):\n",
    "    \"\"\"Extract mel spectrogram from audio.\"\"\"\n",
    "    # Compute mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, \n",
    "        sr=sr, \n",
    "        n_mels=n_mels,\n",
    "        n_fft=n_fft, \n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    \n",
    "    # Convert to log scale (dB)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    return log_mel_spec\n",
    "\n",
    "# Let's visualize a spectrogram to understand what we're working with\n",
    "def plot_spectrogram(spec, title=None):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(spec, x_axis='time', y_axis='mel', cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, sr=22050, duration=5, n_mels=128, \n",
    "                 n_fft=2048, hop_length=512, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.transform = transform\n",
    "        self.cache = {}  # Cache spectrograms to avoid recomputing\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Use cached spectrogram if available\n",
    "        if audio_path in self.cache:\n",
    "            spec = self.cache[audio_path]\n",
    "        else:\n",
    "            # Load audio\n",
    "            try:\n",
    "                audio, sr = librosa.load(audio_path, sr=self.sr)\n",
    "                \n",
    "                # Ensure consistent duration\n",
    "                target_length = self.duration * sr\n",
    "                if len(audio) < target_length:\n",
    "                    # Pad if too short\n",
    "                    audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "                else:\n",
    "                    # Trim if too long\n",
    "                    audio = audio[:target_length]\n",
    "                \n",
    "                # Extract spectrogram\n",
    "                spec = extract_melspectrogram(\n",
    "                    audio, sr, n_mels=self.n_mels, \n",
    "                    n_fft=self.n_fft, hop_length=self.hop_length\n",
    "                )\n",
    "                \n",
    "                # Cache for future use\n",
    "                self.cache[audio_path] = spec\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {audio_path}: {e}\")\n",
    "                # Return empty spectrogram on error\n",
    "                spec = np.zeros((self.n_mels, int((self.duration * self.sr) / self.hop_length) + 1))\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            spec = self.transform(spec)\n",
    "        \n",
    "        # Convert to tensor (add channel dimension)\n",
    "        spec_tensor = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Shape: [1, n_mels, time]\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return spec_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes, n_mels=128):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Calculate the size of the flattened features\n",
    "        # This depends on your input spectrogram size and pooling layers\n",
    "        # For n_mels=128 and 5-second audio, after 4 pooling layers of factor 2:\n",
    "        # Height: 128 / (2^4) = 8\n",
    "        # Width: depends on hop_length, but roughly 5*sr/hop_length / (2^4)\n",
    "        # For sr=22050 and hop_length=512: 5*22050/512 / 16 ≈ 13\n",
    "        self.flat_features = 256 * 8 * 13  # Adjust based on your actual dimensions\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flat_features, 512)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, self.flat_features)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Loading metadata...\n",
      "Total number of species: 206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding files: 100%|██████████| 500/500 [00:00<00:00, 25641.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 valid files\n",
      "Class distribution:\n",
      "  grekis: 20 files\n",
      "  amekes: 13 files\n",
      "  compau: 13 files\n",
      "  soulap1: 13 files\n",
      "  gycwor1: 12 files\n",
      "  ... and 123 more classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "print(\"Preparing dataset...\")\n",
    "file_paths = []\n",
    "labels = []\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"/data/birdclef/birdclef-2025\"\n",
    "train_audio_dir = os.path.join(base_dir, \"train_audio\")\n",
    "train_csv_path = os.path.join(base_dir, \"train.csv\")\n",
    "taxonomy_path = os.path.join(base_dir, \"taxonomy.csv\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = 'training_results/mel_cnn_model_2'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "print(\"Loading metadata...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "taxonomy_df = pd.read_csv(taxonomy_path)\n",
    "\n",
    "# Increase sample size\n",
    "sample_size = 500  # Adjust based on your needs\n",
    "sampled_df = train_df.sample(sample_size, random_state=42)\n",
    "# sampled_df = train_df\n",
    "\n",
    "# Create a mapping from primary_label to class index\n",
    "unique_labels = train_df['primary_label'].unique()\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "print(f\"Total number of species: {len(unique_labels)}\")\n",
    "\n",
    "for _, row in tqdm(sampled_df.iterrows(), total=len(sampled_df), desc=\"Finding files\"):\n",
    "    species_id = row['primary_label']\n",
    "    filename = row['filename']\n",
    "    file_path = os.path.join(train_audio_dir, filename)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        file_paths.append(file_path)\n",
    "        labels.append(label_to_idx[species_id])\n",
    "\n",
    "print(f\"Found {len(file_paths)} valid files\")\n",
    "\n",
    "# Filter out classes with too few samples\n",
    "label_counts = Counter(labels)\n",
    "print(\"Class distribution:\")\n",
    "for label, count in label_counts.most_common(5):\n",
    "    print(f\"  {idx_to_label[label]}: {count} files\")\n",
    "print(f\"  ... and {len(label_counts) - 5} more classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    file_paths, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SpectrogramDataset(train_files, train_labels)\n",
    "val_dataset = SpectrogramDataset(val_files, val_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = Counter(train_labels)\n",
    "total_samples = len(train_labels)\n",
    "class_weights = {cls: total_samples / (len(class_counts) * count) for cls, count in class_counts.items()}\n",
    "weight_tensor = torch.FloatTensor([class_weights.get(i, 1.0) for i in range(len(unique_labels))]).to(device)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = AudioCNN(num_classes=len(unique_labels)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: Train Loss=5.5113, Val Loss=6.0547, Accuracy=0.0600\n",
      "Saved results and model checkpoint at epoch 1\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize results tracking\n",
    "results = {\n",
    "    'epochs': [],\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'accuracy': [],\n",
    "    'timestamp': []\n",
    "}\n",
    "\n",
    "# Function to find the latest checkpoint\n",
    "def find_latest_checkpoint(results_dir):\n",
    "    checkpoints = [f for f in os.listdir(results_dir) if f.startswith('cnn_model_epoch_') and f.endswith('.pt')]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    \n",
    "    # Extract epoch numbers and find the highest\n",
    "    epoch_nums = [int(f.split('_')[-1].split('.')[0]) for f in checkpoints]\n",
    "    if not epoch_nums:\n",
    "        return None\n",
    "    \n",
    "    latest_epoch = max(epoch_nums)\n",
    "    return os.path.join(results_dir, f'cnn_model_epoch_{latest_epoch}.pt')\n",
    "\n",
    "# Check if we have a checkpoint to resume from\n",
    "latest_checkpoint = find_latest_checkpoint(results_dir)\n",
    "start_epoch = 0\n",
    "\n",
    "# Initialize model, criterion, optimizer\n",
    "model = AudioCNN(num_classes=len(unique_labels)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Initialize results tracking\n",
    "results = {\n",
    "    'epochs': [],\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'accuracy': [],\n",
    "    'timestamp': []\n",
    "}\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "if latest_checkpoint:\n",
    "    print(f\"Resuming from checkpoint: {latest_checkpoint}\")\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "    # Also load results if available\n",
    "    results_file = os.path.join(results_dir, f'cnn_results_epoch_{start_epoch}.json')\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'r') as f:\n",
    "            results = json.load(f)\n",
    "    \n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"Starting training from scratch\")\n",
    "\n",
    "# Modify training loop to start from the correct epoch\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (specs, labels) in enumerate(tqdm(train_loader, \n",
    "                                                     desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "                                                     leave=False)):\n",
    "        specs, labels = specs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(specs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for specs, labels in val_loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    correct = sum(1 for x, y in zip(all_preds, all_labels) if x == y)\n",
    "    total = len(all_labels)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Update learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store results\n",
    "    results['epochs'].append(epoch + 1)\n",
    "    results['train_loss'].append(float(train_loss))\n",
    "    results['val_loss'].append(float(val_loss))\n",
    "    results['accuracy'].append(float(accuracy))\n",
    "    results['timestamp'].append(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "    \n",
    "    # Save results and model periodically\n",
    "    if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "        results_file = os.path.join(results_dir, f'cnn_results_epoch_{epoch+1}.json')\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        model_file = os.path.join(results_dir, f'cnn_model_epoch_{epoch+1}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'accuracy': accuracy\n",
    "        }, model_file)\n",
    "        \n",
    "        print(f\"Saved results and model checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
